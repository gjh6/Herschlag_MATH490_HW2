{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Experimental design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are rummaging in an attic and come across an old weighted coin.  The coin is accompanied with an decaying pamphlet which states the coin is weighted to come up on one side 60% of the time.  From holding the coin, you can't tell which is the favored side.\n",
    "\n",
    "Before you start flipping the coin, you wish to estimate the number of times you will need to flip the coin in order to be $99.85$% sure you have identified each side of the coin with the correct probability (i.e. $\\text{erf}(3)$, or one-sided three standard deviations).\n",
    "\n",
    "1. Perform an estimate using the central limit theorem.\n",
    "2. Perform an estimate using Cramer's large deviation theorem.\n",
    "3. Perform an estimate using numerical simulation.\n",
    "4. When compared with the numerics, did the two theorems give over or under estimates?\n",
    "5. (Non-rigorous question) What are some ideas you have on why the theorems differed from practice?  Don't feel like you have to be too rigorous in answering this question, but feel free to try to figure this out with some extra reading, some conjectures, or some additional numerical experimentation.  In particular, we are going to examine the descrepancy in Cramer's Theorem below.\n",
    "  \n",
    "*Hint:* If helpful, a restatement of Cramer's theorem states that \n",
    "\n",
    "* $\\lim_{N\\rightarrow\\infty} \\frac{1}{N} \\log(\\mathbf{P}(\\bar{f}_N \\geq a) = -\\Lambda^*(a)$, where $\\bar{f}_N$ is the sample mean, $a>\\mathbb{E}[f]$, and $\\Lambda^*(\\lambda)$ is the Legendre transform of $\\log(\\mathbb{E}[e^{X\\lambda}])$. Here the Legendre transform is the negative of how we defined it in week 2 of class (see week 2, day 2 notes an also the comment at the beginning of week 3, day 1 notes). \n",
    "* Also $\\lim_{N\\rightarrow\\infty} \\frac{1}{N} \\log(\\mathbf{P}(\\bar{f}_N \\leq a) = -\\Lambda^*(a)$, where $\\bar{f}_N$ is the sample mean, $a>\\mathbb{E}[f]$, and $\\Lambda^*(\\lambda)$ is the Legendre transform of $\\log(\\mathbb{E}[e^{X\\lambda}])$.\n",
    "\n",
    "For more details see [here](https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_theorem_(large_deviations)#:~:text=Cram%C3%A9r's%20theorem%20is%20a%20fundamental,by%20Harald%20Cram%C3%A9r%20in%201938.)\n",
    "\n",
    "*Hint:* When performing numerical experiments, it will be easier to use the binomial distribution with $N$ trials than to run a Burnoulli experiment $N$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Investigating Cramer's Theorem Numerically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further investigate the discrepancies you found in Cramer's thoerem\n",
    "\n",
    "1. Write a function that takes $N$ as an argument and generates a sample mean, $\\bar{f}_N$, from a Bernoulli random variable with $p=0.4$ (you likely did something similar to above; if you haven't take this as a hint for the above problem).\n",
    "2. Write a routine that calls the above function and samples $\\bar{f}_N$.  Show that $\\sqrt{N}(\\bar{f}_N - \\mathbb{E}(f))$ approaches a Guassian distribution both through a direct visualization and a quantile-quantile (QQ) plot.\n",
    "  - A quantileâ€“quantile (QQ) plot is a plot of the quantiles (i.e. the inverse of the cumulative distribution function) of two 1-dimensional distributions against one another. If the resulting curve is y = x, the distributions are the same. This is most often used when at least one of the distributions is empirical (i.e. a collection of samples) and you want to know how close those samples are to some specific distribution. Produce QQ plots to accompany your histograms.\n",
    "  - A tool that might help is the notebook provided from week 2, day 2 when visualizing the central limit theorem.\n",
    "3. Next, write a routine that constructs an estimate $Q_N$ of the probability $p_N = P(\\bar{f}_N > 0.5)$ (to do this generate many samples of $\\bar{f}_N$, much like you did in the previous exercise, but now for various values of $N$). \n",
    "4. Taking a log of the probabilities, plot the prediction of Cramer's theorem (i.e. $-n\\gamma(\\epsilon)$) against what you observe numerically.  Are your finding surprising?  Do they agree with or contradict Cramer's theorem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially helpful ploting calls\n",
    "\n",
    "def logDecay(x):\n",
    "    return -flips*Legendre_Transform_at_oneHalf\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=rangeOfFlips, y=np.log(fractionOfWrongPredictions),\n",
    "                    mode='lines+markers',\n",
    "                    name='Fraction of wrong predictions(numerical)'))\n",
    "fig.add_trace(go.Scatter(x=rangeOfFlips, y=[logDecay(flips) for flips in rangeOfFlips],\n",
    "                    mode='lines',\n",
    "                    name='Prediction from Cramer\\'s theorem'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Investigating Cramer's Theorem (Analytically)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for the descrepancies above are due to sub-exponential terms.  For an illustration of this see [this answer](https://math.stackexchange.com/a/2366311) on math.stackexchange.  \n",
    "\n",
    "We may approximate the scaled binomial distribution we have used above as a normal distribution and then use the bounds and estimates from the above post.  Note that $\\bar{f}_N$ is approximately distributed as a normal distribution with mean $0.4$ and variance $\\sigma^2 = \\frac{0.4(1-0.4)}{\\sqrt{N}}$.\n",
    "\n",
    "Let $X\\sim \\mathcal{N}(0.4, \\frac{0.4(1-0.4)}{\\sqrt{N}})$. \n",
    "\n",
    "Analytically, $P(\\bar{f}_N\\approx X > 0.5) = \\int_{0.5}^{\\infty} \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-(x-0.4)^2/(2 \\sigma^2)} dx = \\int_{0.1 \\sqrt{n}/(0.4(1-0.4))}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} e^{-x^2/2} dx = \\text{erf}(0.1 \\sqrt{n}/(0.4(1-0.4))$\n",
    "\n",
    "From the post above, note that $\\text{erf}(a) < \\frac{1}{a \\sqrt{2 \\pi}} e^{-a^2/2}$ (as a bonus, prove this through) and that this upper bound is fairly tight, i.e. $\\text{erf}(a) \\approx \\frac{1}{a \\sqrt{2 \\pi}} e^{-a^2/2}$\n",
    "\n",
    "1. Use Cramer's Theorem to estimate $\\log(P(X > 0.5))$.\n",
    "2. Confirm that Cramer's theorem agrees with $\\log(\\frac{1}{a \\sqrt{2 \\pi}} e^{-a^2/2})$ in the limit of large $N$, where $a = 0.1 \\sqrt{n}/(0.4(1-0.4))$.\n",
    "3. Examine at the relative error in probability between the estimate from Cramer's theorem and the analytic approximation of $\\text{erf}(a)$.  What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
